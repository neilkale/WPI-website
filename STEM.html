<!DOCTYPE html>
<html lang="en" class="page">

<head>
  <meta charset="UTF-8">
  <link rel="stylesheet" type="text/css" href="styleSheets/myStyle.css">
  <title>STEM</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="text/javascript" src="js/tabs.js"></script>
  <link rel="icon" href="images/favicon.png" sizes="16x16" type="image/png">
</head>

<!-- Title and Nav Bar -->

<body>

  <div>

    <ul>
      <li><a href="personal.html">Why I Do It</a></li>
      <li><a href="index.html">What I Do at School</a></li>
      <li><a href="http://www.sunnysideup.life/sunnysideup.life/index.html">What I Think</a></li>
      <li class="title"> STEM </li>
    </ul>

  </div>

  <div class="formal">

    <p class="title">
      Exploring Emotional Sentience Through AI-Created Music
    </p>

    <p>
      <span class="bold"> Research Question: </span> Can AI express emotional sentience through the music it creates?
    </p>

    <p>
      <span class="bold"> Purpose: </span> Musicality is spread throughout the natural world. However, humans alone use music to convey emotion. This stems from our ability to empathize. If algorithm-produced music can convey emotion, then the
      algorithm steps towards rudimentary sentience.
    </p>

    <!-- Tab links -->
    <div class="tab">
      <button class="tablinks" onclick="openTab(event, 'Poster PDF')">Poster PDF</button>
      <button class="tablinks" onclick="openTab(event, 'References')">References</button>
      <button class="tablinks" onclick="openTab(event, 'Conclusions')">Conclusions</button>
      <button class="tablinks" onclick="openTab(event, 'Discussion')">Discussion</button>
      <button class="tablinks" onclick="openTab(event, 'Results')">Results</button>
      <!-- <div class="dropdown"> -->
      <button class="tablinks" onclick="openTab(event, 'Figures and Tables')">Figures and Tables</button>
      <button class="tablinks" onclick="openTab(event, 'Procedure')">Procedure</button>
      <button class="tablinks" onclick="openTab(event, 'Background')">Background</button>
      <button class="tablinks" onclick="openTab(event, 'Abstract')">Abstract</button>
      <button class="tablinks" onclick="openTab(event, 'Overview')">Overview</button>
      <button class="tablinks" onclick="openTab(event, 'joinUs')">Join Us</button>
    </div>

    <div id="Poster PDF" class="tabcontent">
      <h3>Poster PDF</h3>
      <div class="clearfix">
        <embed src="docs/STEM1_Poster.pdf" type="application/pdf" class="pdf">
      </div>
    </div>

    <div id="Figures and Tables" class="tabcontent">
      <h3>Figures and Tables</h3>
      <div class="clearfix">
        <embed src="docs/figures_and_tables_STEM1.pdf" type="application/pdf" class="pdf">
      </div>
    </div>

    <div id="Discussion" class="tabcontent">
      <h3>Discussion</h3>
      <h4>Conscious Differentiation Not Found</h4>
      <p>There was no significant correlation between song composer and how people responded to the discrimination task (p = 0.183). Hence, in general, participants could not consciously differentiate between the human and AI-produced pieces. This demonstrates that modern AI-produced music is able to emulate human compositions in both style and complexity, at least to the point where listeners can no longer readily tell the difference. Since human-produced music and AI-produced music are consciously indifferentiable, further study into how they each impact listeners&rsquo; emotions was warranted. This leads to the next finding.</p>
      <h4>Emotional Effect of AI-Produced Music</h4>
      <p>In general, the emotional data collected supports the hypothesis that AI-produced music can affect the emotions of the listener. However, there is much more to the story. Human and AI music both reduce listeners&rsquo; anxiety and anger significantly. That said, while all 3 human-produced pieces reduced listeners&rsquo; sadness, only 1 out of 4 AI produced pieces managed to do so. This suggests that through certain emotions, such as sadness, humans can subconsciously differentiate between AI and human music.</p>
    </div>

    <div id="Conclusions" class="tabcontent">
      <h3>Conclusion</h3>
      <p>Ultimately, we have found that AI-produced music is not only indifferentiable from human compositions, but also capable of affecting listeners&rsquo; emotions. Besides furthering the theory of AI sentience, this research opens the door to a diverse set of future explorations. In the field of computing, further study must be done to determine what aspects of the algorithm cause certain emotions to be evoked. This may involve comparing algorithms that successfully manipulate an emotion with those that do not. In the field of behavioral science and psychology, further study may involve examining brain wave activity and heart rate while individuals listen to AI-composed music. This research has found that there are differences in the effect of AI-produced music and human-produced music. These differences are unnoticeable at the conscious level, and only somewhat revealed by emotions. Perhaps greater insights lie at a more fundamental level such as the firing of neurons, or the rhythm of the heart.
      </p>
    </div>

    <div id="Abstract" class="tabcontent">
      <h3>Abstract</h3>
      <p>As the 21st century progresses, artificial intelligence is stepping intellectually closer to mankind than ever before. That said, until now, there was a certain essence of humanity which many believed was still missing from machines, namely the power to create and the power to emote. In recent times, AI has been developed with the ability to create music. If humans listen to AI-composed and human-composed music, they are unable to tell the difference and are emotionally affected by both. To demonstrate this hypothesis, a discrimination task was administered wherein participants differentiated between human and AI-produced music. Throughout the experiment, the Discrete Emotional Questionnaire was used to collect self-reported data on the pieces&rsquo; impact on listeners&rsquo; emotional state.</p>
      <p>There was no significant difference in discrimination task response depending on whether the piece was human-written or AI-written, X2 (2, N=156) = 3.44, p = 0.183. In other words, participants could not distinguish the two. Furthermore, both human and AI-produced music reduced listener anxiety and anger. However, more human-produced pieces of music affected the listeners&rsquo; sadness and relaxation.</p>
      <p>Humans and AI both have the capacity to emotionally affect through music. If sentience includes the ability to express emotion, AI has stepped closer to humans in that regard. As a result of these findings, further research is warranted on multiple fronts. Computationally, to determine what specific code allows an algorithm to elicit certain emotional responses. Biologically, to discover why certain emotions, and their neural circuits, are more affected by AI music than others.</p>
    </div>

    <div id="Overview" class="tabcontent">
      <h3>Overview</h3>
      <p><span style="font-weight: 400;">The overarching question that this project aims to answer is, &ldquo;Can AI express emotional sentience through music?&rdquo; Humans express their emotions in many forms including art, literature, speech,
          facial expressions, and music. As a music lover, I will explore whether AI can match humans in expressing emotion through music.</span></p>
      <p><span style="font-weight: 400;">I propose a series of hypotheses to test this question.</span></p>
      <p><span style="font-weight: 400;">First, if a creative algorithm is trained or modelled on human music, then the resultant compositions will be indistinguishable from human compositions. Humans propagate their emotions through music (Juslin,
          2013). Therefore, if a generative algorithm can produce music that is indistinguishable from that produced by humans, then the algorithm can allow an AI to convey emotions. Finally, one of the indicators of sentience is the ability to
          propagate one&rsquo;s feelings (Keltner et. al, 2019). If an algorithm allows AI to convey its emotions, then the AI may be one step closer to sentience.</span></p>
    </div>


    <div id="joinUs" class="tabcontent">
      <h3>Join Us</h3>
      <p>
        To participate, simply print and fill out the informed consent form (see tab below) and send a picture/scan to nakale@wpi.edu. You will then recieve the password to the survey (linked below), along with further instructions.
      </p>
      <div class="tab">
        <button class="tablinks" onclick="openTab(event, 'STEMSurvey')">STEM Survey</button>
        <button class="tablinks" onclick="openTab(event, 'InformedConsent')">Informed Consent Form</button>
      </div>
    </div>

    <div id="STEMSurvey" class="tabcontent">
      <h3>STEM Survey</h3>
      <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSehnQI2tAE6bZ4yQZeqCHmcHYYDalXGpjYFmp-McYVO1ytiWg/viewform?embedded=true"></iframe>
    </div>

    <div id="InformedConsent" class="tabcontent">
      <h3>Informed Consent Form</h3>
      <div class="clearfix">
        <embed src="docs/informedConsent.pdf" type="application/pdf" class="pdf">
      </div>
    </div>


    <div id="Background" class="tabcontent">
      <h3>Background</h3>
      <p dir="ltr">&nbsp;&nbsp;&nbsp;&nbsp;What defines sentience? The word derives from the Latin root sentire which means“to sense or to feel”, and indeed sentience is to be “responsive to or
        conscious of sense impressions” (Merriam-Webster, n.d.). In other words, it’s simply the ability to sense one’s surroundings. However, a thermometer can sense the temperature around it, yet we wouldn’t call it sentient. A light switch can
        sense whether it is on or off, hold that information, and even send it to a connected bulb. That said, it is still not sentient. Perhaps, what makes humans unique, and sentient, is not our ability to feel, but rather our ability to share
        feelings with others (Keltner et. al, 2019). By the status quo, computers aren’t sentient (Allen, 2016). However, if AI can read human music that expresses emotion and machine learning allows AI to create unique compositions modelled on
        whatever it reads, then I believe that artificial-intelligence created music will propagate emotions as well. The purpose of this project is to explore whether AI can use music to express emotion as well as humanity can.</p>
      <p>
      </p>
      <p dir="ltr">&nbsp;&nbsp;&nbsp;&nbsp;Humans express their emotions through a diverse set of mediums such as facial expression, speech, literature, music, and art. This project specifically
        focuses on whether algorithmic music compositions can induce an emotional response in humans. If so, an AI equipped with these algorithms is capable of propagating emotions, and we are one step closer to asking the question - “Can AI be
        sentient?”&nbsp;</p>
      <p>
      </p>
      <p dir="ltr">&nbsp;&nbsp;&nbsp;&nbsp;There remains the question of whether these emotions are truly “its own” if the AI is merely selecting its emotion based off of an algorithm. However, I
        argue that humans also derive their emotions from internal algorithms, albeit extremely complicated ones. This doesn’t reduce the value of human emotions or sentience. In fact, it is all the more impressive that our complex emotions like love
        and happiness can arise from an algorithm, rather than spontaneously materializing.</p>
      <p>
      </p>
      <p dir="ltr">&nbsp;&nbsp;&nbsp;&nbsp;If the human mind is a complex algorithm, then our emotional expression is also algorithmic. The great works of Mozart, Michaelangelo, Shakespeare, and
        Van Gogh were not spontaneous feats of creativity; they were algorithmically generated by the minds of their creators. The difference between AI and humans may simply be the complexity of their algorithms.&nbsp;</p>
      <p>
      </p>
      <p dir="ltr">&nbsp;&nbsp;&nbsp;&nbsp;Ultimately, as we progress through the 21st century, the line between human and AI is fading. Advances in neuroscience are showing that we may be more
        algorithmic than we previously believed (Yayilgan &amp; Beachell, 2006). Furthermore, machine learning is allowing AI to step closer to humanity than ever before (Shabbir &amp; Anwer, 2018). This project enters the no-man’s land (and no-AI’s
        land) between humanity and AI, exploring what it means to be sentient as well as what it means to be human.</p>
    </div>

    <div id="Procedure" class="tabcontent">
      <!-- <h3>Procedure</h3>

      <p>
        The plan that I developed prior to conducting research.
      </p>

      <div class="tab">
        <button class="tablinks" onclick="openTab(event, 'RiskAndSafety')">Risk and Safety</button>
        <button class="tablinks" onclick="openTab(event, 'HumanParticipants')">Human Participants Research</button>
        <button class="tablinks" onclick="openTab(event, 'ProcedureOverview')">Procedure Overview</button>
      </div> -->
      <h3> Procedure </h3>
      <h4>Consent</h4>
<p>All participants were required to complete a consent form prior to testing. Those under 18 were required to obtain parental consent as well. The form outlined the rationale behind this project, the testing procedure, as well as the potential risks and returns for participants. A copy can be found in Appendix x.</p>
<h4>Recruitment</h4>
<p>The survey population was Juniors attending the Massachusetts Academy of Math and Science. Students were recruited via word of mouth and email. All participation was purely voluntary with no tangible returns.</p>
<h4>Study Interface</h4>
<p>Testing took place at the Massachusetts Academy of Math and Science. Survey participants were seated in a room reserved for testing, with each taking the survey individually on his/her personal computer.</p>
<p>Participants were asked to go to <a href="http://www.mamsmag.wordpress.com">www.mamsmag.wordpress.com</a>, the school magazine website. From there, they navigated to the survey, which was in the form of an embedded Google form. Music was accessible through Google Drive URLs placed in the survey, which linked to the audio files.</p>
<p>There was no contact between the experimenter and the participants during the survey, excepting assistance with technical difficulties such as malfunctioning headphones.</p>
<h4>Music Selection</h4>
<p>All music pieces were in MIDI format to maintain consistency in how the music was played. AI-composed pieces were produced using AIVA, a stochastic music-generation algorithm. A link to the AIVA website is provided in the references and in Appendix x. Human-composed pieces were selected randomly from an online database of open-source MIDI songs. This database is also linked in Appendix x. In terms of style, a varied sampling of both human and AI music was selected in order to better represent AI and human music at large.</p>
<h4>Survey</h4>
<p>First, participants were asked to answer several demographic questions. These included gender, age, questions about musical experience, and a self-assessment of familiarity with music. Next, participants took the Discrete Emotion Questionnaire (DEQ), a set of questions designed to measure emotional state through self-report, to establish an emotional baseline. After this, the first piece of music was revealed. Participants were asked to listen to the piece then state whether they believed it was written by a human, an AI, or that it was indistinguishable. They then took the DEQ once more to measure any changes in emotional state. This pattern of music reveal, discrimination task, and DEQ was repeated for each subsequent piece of music.</p>
<h4>Data Analysis</h4>
<p>Data from the discrimination task was grouped into two categories: AI-composed and human-composed. It was then tested for significance via a Chi-Square Analysis. Data collected from the repeated DEQs was first tested using a one-way ANOVA to determine whether significant variance existed between iterations. If so, Dunnett&rsquo;s Test was used for post-hoc analysis. The T-test was not used to avoid compounding Type-1 error.</p>
    </div>

    <!-- <div>

          <p>Data Analysis: For each composition, statistical analysis will determine whether there is a significant difference between the number of people who believed that the song was written by a human, and the number who believed that it was
            written by a robot. The analysis will also look at the total percentage of correct responses for human pieces, and the percentage of correct responses for algorithmic compositions. A comparison can also be done between songs of different
            genres. Additional analysis could include dividing the responses by participant demographics to determine whether certain groups chose the correct response more often than others.</p>

          <p>Discussion of Results and Conclusions: Humans use music to propagate their emotions. Therefore, if this survey finds that certain algorithm generated pieces are indistinguishable from human creations, those algorithms could allow AI to
            convey its emotions to humans. The ability to propagate one&rsquo;s feelings to others is one indicator of sentience which suggests that AI could possibly reach that mark . Furthermore, the question arises: Which algorithms correlate to
            which
            emotions?</p>

          <p>If the survey determines that algorithmic pieces are distinguishable from human pieces, it can be concluded that algorithms can&rsquo;t induce emotions via music in the same way that humans can. By studying the timestamps on when people
            made
            their decisions while listening to the songs, I may be able to determine what allowed people to distinguish. Perhaps there is a fundamental aspect of emotional expression via music that AI will never be able to capture; however, I believe
            that the difference will reduce over time as technology advances.</p>

          <p>Future Extensions: If I have the opportunity, I would like to run this experiment with music ensemble members at WPI. This would allow me to test whether there is a correlation between music experience and ability to discriminate between
            human and AI music.</p>

          <p>Given time, I would also like to directly measure how people are emotionally affected by AI-written music. One possible method would be having people listen to short samples of human created music, and select, off of a list of basic
            emotions, which emotions are evoked while they listen to that piece. I could feed those samples into an algorithm which can extend them (One such example is Google Magenta), and see whether when those same participants listen to an
            extended
            piece, or samples thereof, they select the same emotion/s for it that they selected for the original sample which created it. Another possible method is to utilize an EEG scanner. Participants will wear the scanner while listening to
            human
            written pieces and algorithm written pieces. The scans will be overlaid to see whether brain activation differs between the two data sets.</p>

          <p>In the future, I could also run similar surveys and/or EEG scans for human/algorithm-created artwork, literature, speech, or facial expressions. These are all other methods which humans use to express emotion. Determining whether
            algorithms
            can replicate one more effectively than another might provide valuable insight into how algorithms evoke emotion, as well as into how we humans perceive it.</p>
        </div> -->

    <div id="ProcedureOverview" class="tabcontent">
      <h3>Procedure Overview</h3>
      <p>First, compile 15-second to 1-minute samples of human-written and algorithm-written music. The samples will be MIDI files, to eliminate inconsistencies in human playing. The songs will encompass multiple genres including pop and
        classical. Second, develop and disseminate a survey testing whether humans can distinguish the human compositions from algorithmic compositions. The songs will be presented one at a time. While/after the song plays, the participant will be
        able to select one of three buttons: human, AI, indistinguishable. The responses will be compiled into a spreadsheet. See &ldquo;Human Participants Research&rdquo; section for a more detailed overview of the survey.</p>
    </div>

    <div id="RiskAndSafety" class="tabcontent">
      <h3>Risk and Safety</h3>
      <p>The music shouldn&rsquo;t be played loud enough to cause auditory damage to a listener. To this end, participants will be able to control the volume of their device while taking the survey. Additionally, the discord in
        some compositions may cause auditory discomfort to participants. To counteract this, the participant will be free to exit the survey at any time. Finally, to preserve participants&rsquo; anonymity, personal data questions will be optional.
      </p>
    </div>

    <div id="HumanParticipants" class="tabcontent">
      <h3>Human Participants Research</h3>
      <p>The survey population is Juniors attending the Massachusetts Academy of Math and Science. These students will be recruited via word of mouth and a notice posted to the school library bulletin board.</p>

      <p>The first portion of the survey collects personal data such as age and experience with music. After that, the participant is instructed to listen to pieces of music, which will be played to the entire group, and, if possible, discern whether
        the piece is written by a human or a robot. The pieces will play one at a time.</p>

      <p>During each piece, the user will select one of three options - &ldquo;human-written&rdquo;, &ldquo;robot-written&rdquo;, &ldquo;indistinguishable&rdquo;. The piece will only play once. Once the user is satisfied with his/her response, he/she
        can submit, and the next piece will play. I may run multiple iterations of the survey, each with different pieces. Each participant may only take each iteration once. The survey should take no longer than 10 minutes, and the participant is
        free to exit at any time.</p>

      <p>The risks and corresponding countermeasures, as stated in the experimental design, are as follows: Prolonged exposure to loud music can cause hearing damage. To prevent any risk during experimentation, a test sound will be played prior to
        testing, and lowered until all participants are satisfied. The volume of the music will not exceed the volume of the test sound. Additionally, the discord in some compositions may cause auditory discomfort to participants. To counteract this,
        the participant will be free to exit the survey at any time. One benefit is that participants who enjoy music might enjoy this exercise. Additionally, being able to differentiate between AI and human created forms of emotional expression may
        become a vital skill in the future. Already, telling &ldquo;deep fakes&rdquo; apart from real videos of celebrities and politicians is important to prevent misinformation. As AI becomes more emotionally sophisticated, it may become difficult
        to differentiate between a real Beethoven symphony and an algorithmic &ldquo;deep fake&rdquo;. This survey serves as a practice round for would-be connoisseurs of human music. Any personal data collected by the survey will be anonymous. There
        will be no name, email, or phone number attached to the data. Moreover, it will all be collected online. The data may include: age, gender, economic background, and/or experience with music. The data will be stored in an Excel spreadsheet
        linked to my WPI Microsoft account. The account is password-locked, and the only people with the password are me and the school tech administrator. I will keep the data after the study has concluded, in case it is needed for future research.
        I will inform participants about the purpose of the study, what they will be asked to do, that their participation is voluntary and they have the right to stop at any time through a mandatory informed consent form which will outline all of
        that information.</p>
    </div>

    <div id="Results" class="tabcontent">
      <h3>Results</h3>
<h4>Discrimination Task</h4>
<p>A Chi-Square analysis (See Appendix x) was conducted on the data from the discrimination task. The two variables tested for correlation were responses to the discrimination task (Human / AI / Indistinguishable) and whether or not the composer was human (Human-composed / AI-composed). The test resulted in a p-value of 0.183 (X2 = 3.44, DF = 2). We fail to reject the null hypothesis which states that whether or not the composer was human has no effect on the responses to the discrimination task.&nbsp;</p>
<h4>Post-Music Emotional State</h4>
<p>Single-Factor ANOVA was conducted independently for each emotional category from the Discrete Emotion Questionnaire. The categorical independent variable was the composition that was listened to just prior to taking the DEQ. The dependent variable was the extent to which the listener felt the emotion. This was quantified as the sum of the responses for all items on the DEQ which fell under that emotional category.&nbsp;</p>
<p>There was a significant effect of musical composition on anger at the p&lt;.05 level for the three conditions [F(7, 174) = 4.436, p = 0.000]. There was not a significant effect of musical composition on disgust at the p&lt;.05 level for the three conditions [F(7, 174) = 1.039, p = 0.406]. There was not a significant effect of musical composition on fear at the p&lt;.05 level for the three conditions [F(7, 174) = 1.951, p = 0.064]. There was a significant effect of musical composition on anxiety at the p&lt;.05 level for the three conditions [F(7, 174) = 8.064, p = 0.000]. There was a significant effect of musical composition on sadness at the p&lt;.05 level for the three conditions [F(7, 174) = 2.416, p = 0.022]. There was not a significant effect of musical composition on desire at the p&lt;.05 level for the three conditions [F(7, 174) = 1.878, p = 0.076]. There was a significant effect of musical composition on relaxation at the p&lt;.05 level for the three conditions [F(7, 174) = 4.116, p = 0.000]. There was not a significant effect of musical composition on happiness at the p&lt;.05 level for the three conditions [F(7, 174) = 1.710, p = 0.109].</p>
<p>Happiness, relaxation, and desire were aggregated to produce a single measure of positive emotion. There was not a significant effect of musical composition on positive emotion at the p&lt;.05 level for the three conditions [F(7, 174) = 2.590, p = 0.015]. Similarly, anger, disgust, fear and anxiety were aggregated to produce a single measure of negative emotion. There was a significant effect of musical composition on negative emotion at the p&lt;.05 level for the three conditions [F(7, 174) = 5.875, p = 0.000].</p>
<p>In summary, musical composition was significantly correlated to anger, anxiety, sadness, and relaxation, as well as both aggregated positive and aggregated negative emotion.</p>
<p>Dunnett&rsquo;s Test at a significance level of p = 0.05 determined the following results. Human Pop (p = 0.000), AI Tango (p = 0.010), AI Pop (p = 0.000), Human Blues (p = 0.000), and AI Jazz (p = 0.037) had a significant effect on Anger. All the pieces (See Appendix x) had a significant effect on Anxiety (p = 0.000 for each piece). Human Pop (p = 0.000), Human Rock (p = 0.030), Human Blues (p = 0.014), and AI Electronic (p = 0.025) had a significant effect on Sadness. Only the Human Pop (p = 0.024) had a significant effect on Relaxation. Although the ANOVA found significant differences, Dunnett&rsquo;s found that no individual pieces had a significant effect on Positive Emotion. By contrast, all pieces had a significant impact on Negative Emotion (p &lt;= 0.035).</p>
<h4>Demographic Data</h4>
<p><br />The average participant age was 16.261 (SD = 0.113, n = 23). On average, participants rated their familiarity with music, on a scale from 1 to 10 with 10 being very familiar and 1 being not at all, as 6.739 (SD = 0.316). Approximately 43% of participants were female.</p>
    </div>

    <div id="References" class="tabcontent">
      <h3 class="center">References</h3>
      <p>Allen, A.&nbsp; D.&nbsp; (2016).&nbsp; The forbidden sentient computer: Recent progress in the electronic monitoring of</p>
      <p style="padding-left: 30px;">consciousness.&nbsp; IEEE Access, 4, 5649-5658.&nbsp; doi:10.1109/ACCESS.2016.2607722</p>
      <p>Dowling, W.&nbsp; J., and Harwood, D.&nbsp; L.&nbsp; (1986).&nbsp; Music Cognition.&nbsp; New York, NY: Academic Press</p>
      <p>Edwards, M.&nbsp; (2011).&nbsp; Edwards, M.&nbsp; algorithmic composition: Computational thinking in music.&nbsp;</p>
      <p style="padding-left: 30px;">Retrieved from <a
          href="https://www.openaire.eu/search/publication?articleId=od______3094::bff6ccd36e04a8ad00">https://www.openaire.eu/search/publication?articleId=od______3094::bff6ccd36e04a8ad002e2ce6e6bce6cb</a></p>
      <p>Harmon-Jones, C., Bastian, B., &amp; Harmon-Jones, E.&nbsp; (2016).&nbsp; The discrete emotions questionnaire: A</p>
      <p style="padding-left: 30px;">new tool for measuring state self-reported emotions.&nbsp; PloS One, 11(8), e0159915.&nbsp; doi:10.1371/journal.pone.0159915</p>
      <p>Juslin, P.&nbsp; N.&nbsp; (2013).&nbsp; What does music express? basic emotions and beyond.&nbsp; Frontiers in Psychology,</p>
      <p style="padding-left: 30px;">4, 596.&nbsp; doi:10.3389/fpsyg.2013.00596</p>
      <p>Keltner, D., Sauter, D., Tracy, J., &amp; Cowen, A.&nbsp; (2019).&nbsp; Emotional expression: Advances in basic</p>
      <p style="padding-left: 30px;">emotion theory.&nbsp; Journal of Nonverbal Behavior, 43(2), 133-160.&nbsp; doi:10.1007/s10919-019-00293-3</p>
      <p>Lerdahl, F.&nbsp; (2001).&nbsp; Tonal pitch space.&nbsp; doi:10.2307/40285402</p>
      <p>Lumley, M.&nbsp; A., Neely, L.&nbsp; C., &amp; Burger, A.&nbsp; J.&nbsp; (2007).&nbsp; The assessment of alexithymia in medical</p>
      <p style="padding-left: 30px;">settings: Implications for understanding and treating health problems.&nbsp; Journal of Personality Assessment, 89(3), 230-246.&nbsp; doi:10.1080/00223890701629698</p>
      <p>Mauss, I.&nbsp; B., &amp; Robinson, M.&nbsp; D.&nbsp; (2009).&nbsp; Measures of emotion: A review.&nbsp; Cognition &amp; Emotion,</p>
      <p style="padding-left: 30px;">23(2), 209-237.&nbsp; doi:10.1080/02699930802204677</p>
      <p>McDuff, D., &amp; Czerwinski, M.&nbsp; (2018, Nov 20,).&nbsp; Designing emotionally sentient agents.&nbsp;</p>
      <p style="padding-left: 30px;">Communications of the ACM, 61, 74-83.&nbsp; doi:10.1145/3186591 Retrieved from http://dl.acm.org/citation.cfm?id=3186591</p>
      <p>Merriam-Webster.&nbsp; (2019).&nbsp; Definition of sentience.&nbsp; Retrieved from <a href="https://www.merriam-">https://www.merriam-</a></p>
      <p style="padding-left: 30px;">webster.com/dictionary/sentience</p>
      <p>Mozart, W.&nbsp; A., &amp; Taubert, K.&nbsp; H.&nbsp; (1956).&nbsp; Musikalisches w&uuml;rfelspiel.&nbsp; Mainz [u.a.]: Schott.</p>
      <p>Naar, H.&nbsp; (n.d.).&nbsp; Art and Emotion.&nbsp; Retrieved December 5, 2019, from <a href="https://www.iep.utm.edu/art-emot/">https://www.iep.utm.edu/art-emot/</a>.</p>
      <p>Shabbir, J., &amp; Anwer, T.&nbsp; (2018).&nbsp; Artificial intelligence and its role in near future.&nbsp; Retrieved from</p>
      <p style="padding-left: 30px;"><a href="https://arxiv.org/abs/1804.01396">https://arxiv.org/abs/1804.01396</a></p>
      <p>Shapshak, P.&nbsp; (2018).&nbsp; Artificial intelligence and brain.&nbsp; Bioinformation, 14(1), 38-41.</p>
      <p style="padding-left: 30px;">doi:10.6026/97320630014038</p>
      <p>Smith, R., Dennis, A., &amp; Ventura, D.&nbsp; (2012).&nbsp; Automatic composition from non-musical inspiration</p>
      <p style="padding-left: 30px;">sources.</p>
      <p>Supper, M.&nbsp; (2001).&nbsp; A few remarks on algorithmic composition.&nbsp; Computer Music Journal, 25(1), 48-53.</p>
      <p style="padding-left: 30px;">Retrieved from <a href="https://search.proquest.com/docview/1255386">https://search.proquest.com/docview/1255386</a></p>
      <p>Undurraga, E.&nbsp; A., Emlem, N.&nbsp; Q., Gueze, M., Eisenberg, D., Huanca, T., Reyes-</p>
      <p>Garc&iacute;a, V., &amp; Godoy, R.&nbsp; Musical chord preference: Cultural or universal? data from a</p>
      <p style="padding-left: 30px;">native Amazonian society.</p>
    </div>

  </div>

</body>

</html>
